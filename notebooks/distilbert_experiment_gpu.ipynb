{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c101a2fa",
   "metadata": {},
   "source": [
    "# DistilBERT Experiment with PyTorch (GPU)\n",
    "This notebook demonstrates fine-tuning DistilBERT for comment moderation using PyTorch and GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14279edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de102e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\John\\anaconda3\\envs\\ml2_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import re\n",
    "import unicodedata\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e72bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text):\n",
    "    # Unicode normalization\n",
    "    text = unicodedata.normalize('NFKC', str(text))\n",
    "    # Remove HTML tags/entities\n",
    "    text = html.unescape(re.sub(r'<.*?>', '', text))\n",
    "    # Remove headers/footers and wiki markup\n",
    "    text = re.sub(r'=+.*?=+', ' ', text)  # Remove headers like == Reason ==\n",
    "    text = re.sub(r'REDIRECT\\s+\\S+', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'WP:\\w+', ' ', text)  # Remove WP:SOAPBOX etc.\n",
    "    # Replace usernames/IPs with [USER]\n",
    "    text = re.sub(r'\\b[A-Z][a-z]+[A-Z][a-z]+\\b', '[USER]', text)  # CamelCase usernames\n",
    "    text = re.sub(r'\\b\\d{1,3}(?:\\.\\d{1,3}){3}\\b', '[IP]', text)  # IP addresses\n",
    "    # Remove non-alphanumeric (keep basic punctuation)\n",
    "    text = re.sub(r'[^\\w\\s.,!?\\'\\\"]+', ' ', text)\n",
    "    # Standardize repeated punctuation\n",
    "    text = re.sub(r'([!?.,])\\1+', r'\\1', text)\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b06b1abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (159571, 9)\n",
      "Test shape: (63978, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>moderation_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  moderation_label  \n",
       "0             0        0       0       0              0                 0  \n",
       "1             0        0       0       0              0                 0  \n",
       "2             0        0       0       0              0                 0  \n",
       "3             0        0       0       0              0                 0  \n",
       "4             0        0       0       0              0                 0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load train and test data\n",
    "train_df = pd.read_csv('../data/comments_train.csv')\n",
    "test_df = pd.read_csv('../data/comments_test.csv')\n",
    "print(f'Train shape: {train_df.shape}')\n",
    "print(f'Test shape: {test_df.shape}')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad35ad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to train and test data\n",
    "for df in [train_df, test_df]:\n",
    "    df['comment_text'] = df['comment_text'].apply(lambda x: text_preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42217820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 159571/159571 [00:14<00:00, 11113.38 examples/s]\n",
      "Map: 100%|██████████| 63978/63978 [00:05<00:00, 10771.45 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor(0), 'input_ids': tensor([  101,  7526,  2339,  1996, 10086,  2015,  2081,  2104,  2026,  5310,\n",
      "        18442, 13076, 12392,  2050,  5470,  2020, 16407,  1029,  2027,  4694,\n",
      "         1005,  1056,  3158,  9305, 22556,  1010,  2074,  8503,  2006,  2070,\n",
      "         3806,  2044,  1045,  5444,  2012,  2047,  2259, 14421,  6904,  2278,\n",
      "         1012,  1998,  3531,  2123,  1005,  1056,  6366,  1996, 23561,  2013,\n",
      "         1996,  2831,  3931,  2144,  1045,  1005,  1049,  3394,  2085,  1012,\n",
      "        12997,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize data and prepare datasets for Hugging Face Trainer\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['comment_text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df[['comment_text', 'moderation_label']])\n",
    "test_ds = Dataset.from_pandas(test_df[['comment_text', 'moderation_label']])\n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "test_ds = test_ds.map(tokenize, batched=True)\n",
    "train_ds = train_ds.rename_column('moderation_label', 'labels')\n",
    "test_ds = test_ds.rename_column('moderation_label', 'labels')\n",
    "train_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "print(train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f3e7014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Current device: 0\n",
      "Device count: 1\n",
      "PyTorch default tensor type: cpu\n"
     ]
    }
   ],
   "source": [
    "# Confirm that PyTorch and Transformers are using the GPU\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU name:', torch.cuda.get_device_name(0))\n",
    "    print('Current device:', torch.cuda.current_device())\n",
    "    print('Device count:', torch.cuda.device_count())\n",
    "    print('PyTorch default tensor type:', torch.tensor([1.0]).device)\n",
    "else:\n",
    "    print('WARNING: Training will run on CPU. Install CUDA-enabled PyTorch for GPU support.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4760161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU name: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Set device for GPU usage\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "if device.type == 'cuda':\n",
    "    print('GPU name:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('WARNING: Training will run on CPU. Install CUDA-enabled PyTorch for GPU support.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "232057d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12470' max='12470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12470/12470 59:20, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.065100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.046800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.016700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.009700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.006900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.005500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12470, training_loss=0.02332791756799724, metrics={'train_runtime': 3574.7445, 'train_samples_per_second': 446.384, 'train_steps_per_second': 3.488, 'total_flos': 5.284488817734144e+16, 'train_loss': 0.02332791756799724, 'epoch': 10.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define and train DistilBERT model (force model to GPU if available)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./distilbert_results',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=1000,\n",
    "    fp16=True,  # Enable mixed precision\n",
    "    dataloader_num_workers=4,  # Use more workers for faster loading\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    return {'f1': f1}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ac24c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT F1 weighted: 0.9243 | F1 macro: 0.8097\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95     57735\n",
      "           1       0.54      0.86      0.67      6243\n",
      "\n",
      "    accuracy                           0.92     63978\n",
      "   macro avg       0.76      0.89      0.81     63978\n",
      "weighted avg       0.94      0.92      0.92     63978\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "preds = trainer.predict(test_ds).predictions.argmax(axis=-1)\n",
    "y_test = test_df['moderation_label'].astype(int)\n",
    "f1_w = f1_score(y_test, preds, average='weighted')\n",
    "f1_m = f1_score(y_test, preds, average='macro')\n",
    "print(f'DistilBERT F1 weighted: {f1_w:.4f} | F1 macro: {f1_m:.4f}')\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e7f2ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to distilbert_model_20250813_165325\n"
     ]
    }
   ],
   "source": [
    "# Save model and tokenizer locally (optional)\n",
    "from datetime import datetime\n",
    "ts = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
    "local_model_path = f'distilbert_model_{ts}'\n",
    "model.save_pretrained(local_model_path)\n",
    "tokenizer.save_pretrained(local_model_path)\n",
    "print(f'Model and tokenizer saved to {local_model_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
