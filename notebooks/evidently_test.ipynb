{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66f8176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "from typing import Tuple, Dict, Any, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Evidently v0.6+ API\n",
    "from evidently import Report, Dataset, DataDefinition, BinaryClassification\n",
    "from evidently.presets import DataDriftPreset, ClassificationPreset\n",
    "\n",
    "# Optional Evidently Cloud\n",
    "try:\n",
    "    from evidently.ui.workspace import CloudWorkspace\n",
    "    EVIDENTLY_CLOUD_AVAILABLE = True\n",
    "except Exception:\n",
    "    EVIDENTLY_CLOUD_AVAILABLE = False\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4141c4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "load_dotenv()\n",
    "\n",
    "# You can set API_BASE_URL instead of API_URL if you prefer.\n",
    "API_URL = os.getenv(\"API_URL\", \"http://localhost:8000/predict\").strip()\n",
    "if API_URL.endswith(\"/predict\"):\n",
    "    API_BASE = API_URL.rsplit(\"/\", 1)[0]\n",
    "else:\n",
    "    API_BASE = API_URL  # assume it's already a base\n",
    "\n",
    "API_SINGLE_URL = os.getenv(\"API_SINGLE_URL\", f\"{API_BASE}/predict\").strip()\n",
    "API_BATCH_URL = os.getenv(\"API_BATCH_URL\", f\"{API_BASE}/predict_batch\").strip()\n",
    "\n",
    "ASSETS_DIR = os.getenv(\"ASSETS_DIR\", \"assets\").strip()\n",
    "ORIG_CSV = os.getenv(\"ORIG_TEST_CSV\", os.path.join(ASSETS_DIR, \"comments_test.csv\")).strip()\n",
    "CHGD_CSV = os.getenv(\"CHGD_TEST_CSV\", os.path.join(ASSETS_DIR, \"comments_test_v2.csv\")).strip()\n",
    "\n",
    "# Limit calls for demo. Set SAMPLE_N=None to use full data.\n",
    "SAMPLE_N = int(os.getenv(\"SAMPLE_N\", \"300\")) if os.getenv(\"SAMPLE_N\", \"300\") != \"None\" else None\n",
    "TIMEOUT = float(os.getenv(\"API_TIMEOUT\", \"30\"))\n",
    "BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", \"32\"))\n",
    "\n",
    "# Evidently Cloud (optional)\n",
    "EVIDENTLY_API_KEY = os.getenv(\"EVIDENTLY_API_KEY\")\n",
    "EVIDENTLY_PROJECT_ID = os.getenv(\"EVIDENTLY_PROJECT_ID\")\n",
    "\n",
    "SAVE_DIR = os.getenv(\"SAVE_DIR\", \"monitoring_outputs\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd9bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def load_test(path: str, sample_n: int | None = SAMPLE_N) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    needed = {\"id\", \"comment_text\", \"moderation_label\"}\n",
    "    missing = needed - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"CSV {path} missing columns: {missing}\")\n",
    "    if sample_n and len(df) > sample_n:\n",
    "        df = df.sample(sample_n, random_state=42).reset_index(drop=True)\n",
    "    return df[[\"id\", \"comment_text\", \"moderation_label\"]].copy()\n",
    "\n",
    "\n",
    "def _post_single(text: str) -> Dict[str, Any]:\n",
    "    r = requests.post(API_SINGLE_URL, json={\"text\": str(text)}, timeout=TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "\n",
    "def _post_batch(texts: List[str]) -> List[Dict[str, Any]]:\n",
    "    r = requests.post(API_BATCH_URL, json={\"texts\": [str(t) for t in texts]}, timeout=TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    payload = r.json()\n",
    "    # Expect either a list directly or {\"predictions\": [...]}\n",
    "    if isinstance(payload, list):\n",
    "        return payload\n",
    "    if isinstance(payload, dict) and \"predictions\" in payload:\n",
    "        return payload[\"predictions\"]\n",
    "    raise ValueError(\"Unexpected batch response format\")\n",
    "\n",
    "\n",
    "def call_api_with_fallback(texts: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Try batch; if not available, fall back to single calls.\"\"\"\n",
    "    try:\n",
    "        out: List[Dict[str, Any]] = []\n",
    "        for i in range(0, len(texts), BATCH_SIZE):\n",
    "            chunk = texts[i:i + BATCH_SIZE]\n",
    "            out.extend(_post_batch(chunk))\n",
    "            if (i // BATCH_SIZE + 1) % 5 == 0:\n",
    "                print(f\"  scored {i + len(chunk)}...\", flush=True)\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(f\"Batch endpoint failed, falling back to single calls: {e}\")\n",
    "        out: List[Dict[str, Any]] = []\n",
    "        for i, t in enumerate(texts, 1):\n",
    "            tries = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    out.append(_post_single(t))\n",
    "                    break\n",
    "                except Exception:\n",
    "                    tries += 1\n",
    "                    if tries >= 2:\n",
    "                        raise\n",
    "                    time.sleep(0.4)\n",
    "            if i % 50 == 0:\n",
    "                print(f\"  scored {i}...\", flush=True)\n",
    "        return out\n",
    "\n",
    "\n",
    "def evaluate(df: pd.DataFrame, preds: List[Dict[str, Any]]) -> Tuple[float, float, pd.DataFrame]:\n",
    "    if len(df) != len(preds):\n",
    "        raise ValueError(\"Length mismatch between dataframe and predictions\")\n",
    "\n",
    "    y_pred = [int(p.get(\"moderation_label\", 0)) for p in preds]\n",
    "    max_prob = []\n",
    "    for p in preds:\n",
    "        probs = p.get(\"toxic_probs\", {})\n",
    "        max_prob.append(float(max(probs.values())) if isinstance(probs, dict) and probs else float(\"nan\"))\n",
    "\n",
    "    out = df.copy()\n",
    "    out[\"prediction\"] = y_pred\n",
    "    out[\"score_max_prob\"] = max_prob\n",
    "\n",
    "    y_true = out[\"moderation_label\"].astype(int).values\n",
    "    y_hat = out[\"prediction\"].astype(int).values\n",
    "\n",
    "    f1_w = f1_score(np.asarray(y_true), np.asarray(y_hat), average=\"weighted\")\n",
    "    f1_m = f1_score(np.asarray(y_true), np.asarray(y_hat), average=\"macro\")\n",
    "    print(classification_report(np.asarray(y_true), np.asarray(y_hat), digits=4))\n",
    "    return float(f1_w), float(f1_m), out\n",
    "\n",
    "\n",
    "def build_and_save_reports(\n",
    "    ref_df: pd.DataFrame,\n",
    "    cur_df: pd.DataFrame,\n",
    "    report_name_slug: str,\n",
    "    push_cloud: bool = True,\n",
    "):\n",
    "    required = {\"id\", \"comment_text\", \"moderation_label\", \"prediction\"}\n",
    "    miss_ref = required - set(ref_df.columns)\n",
    "    miss_cur = required - set(cur_df.columns)\n",
    "    if miss_ref or miss_cur:\n",
    "        raise ValueError(f\"Missing columns. Ref missing: {miss_ref}, Cur missing: {miss_cur}\")\n",
    "\n",
    "    def _run_report(_ref: pd.DataFrame, _cur: pd.DataFrame, use_strings: bool):\n",
    "        ref = _ref.copy()\n",
    "        cur = _cur.copy()\n",
    "\n",
    "        if use_strings:\n",
    "            ref[\"moderation_label\"] = ref[\"moderation_label\"].astype(str)\n",
    "            ref[\"prediction\"] = ref[\"prediction\"].astype(str)\n",
    "            cur[\"moderation_label\"] = cur[\"moderation_label\"].astype(str)\n",
    "            cur[\"prediction\"] = cur[\"prediction\"].astype(str)\n",
    "\n",
    "            data_def = DataDefinition(\n",
    "                id_column=\"id\",\n",
    "                text_columns=[\"comment_text\"],\n",
    "                classification=[\n",
    "                    BinaryClassification(\n",
    "                        target=\"moderation_label\",\n",
    "                        prediction_labels=\"prediction\",\n",
    "                        pos_label=\"1\",\n",
    "                        labels=[\"0\", \"1\"],\n",
    "                    )\n",
    "                ],\n",
    "            )\n",
    "        else:\n",
    "            ref[\"moderation_label\"] = ref[\"moderation_label\"].astype(int)\n",
    "            ref[\"prediction\"] = ref[\"prediction\"].astype(int)\n",
    "            cur[\"moderation_label\"] = cur[\"moderation_label\"].astype(int)\n",
    "            cur[\"prediction\"] = cur[\"prediction\"].astype(int)\n",
    "\n",
    "            data_def = DataDefinition(\n",
    "                id_column=\"id\",\n",
    "                text_columns=[\"comment_text\"],\n",
    "                classification=[\n",
    "                    BinaryClassification(\n",
    "                        target=\"moderation_label\",\n",
    "                        prediction_labels=\"prediction\",\n",
    "                        pos_label=1,\n",
    "                        # omit labels to let Evidently infer ints cleanly\n",
    "                    )\n",
    "                ],\n",
    "            )\n",
    "\n",
    "        cols = [\"id\", \"comment_text\", \"moderation_label\", \"prediction\"]\n",
    "        ref_ds = Dataset.from_pandas(ref[cols], data_definition=data_def)\n",
    "        cur_ds = Dataset.from_pandas(cur[cols], data_definition=data_def)\n",
    "\n",
    "        report = Report(metrics=[DataDriftPreset(), ClassificationPreset()])\n",
    "        return report, report.run(reference_data=ref_ds, current_data=cur_ds)\n",
    "\n",
    "    # Try numeric first, then fallback to string labels if Evidently complains\n",
    "    try:\n",
    "        report, eval_result = _run_report(ref_df, cur_df, use_strings=False)\n",
    "    except KeyError:\n",
    "        # mismatch of pos_label vs matrix labels in this build; retry with strings\n",
    "        report, eval_result = _run_report(ref_df, cur_df, use_strings=True)\n",
    "\n",
    "    # Save the report\n",
    "    html_path = os.path.join(SAVE_DIR, f\"{report_name_slug}.html\")\n",
    "    eval_result.save_html(html_path)\n",
    "    print(f\"✓ Saved HTML report to: {html_path}\")\n",
    "\n",
    "    # Cloud upload\n",
    "    if push_cloud and EVIDENTLY_CLOUD_AVAILABLE and EVIDENTLY_API_KEY and EVIDENTLY_PROJECT_ID:\n",
    "        try:\n",
    "            ws = CloudWorkspace(token=EVIDENTLY_API_KEY, url=\"https://app.evidently.cloud\")\n",
    "            project = ws.get_project(EVIDENTLY_PROJECT_ID)\n",
    "            ws.add_run(project.id, eval_result, include_data=False)\n",
    "            print(\"✓ Uploaded run to Evidently Cloud.\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Could not upload to Evidently Cloud: {e}\")\n",
    "    \n",
    "    return eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51da9fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Loading test sets...\")\n",
    "    df_ref = load_test(ORIG_CSV, SAMPLE_N)\n",
    "    df_cur = load_test(CHGD_CSV, SAMPLE_N)\n",
    "\n",
    "    print(f\"Reference rows: {len(df_ref)}\")\n",
    "    print(f\"Changed rows:   {len(df_cur)}\")\n",
    "\n",
    "    # Align on common ids so we compare like for like\n",
    "    common = sorted(set(df_ref[\"id\"]).intersection(set(df_cur[\"id\"])))\n",
    "    if len(common) >= 5:\n",
    "        df_ref = df_ref[df_ref[\"id\"].isin(common)].sort_values(\"id\").reset_index(drop=True)\n",
    "        df_cur = df_cur[df_cur[\"id\"].isin(common)].sort_values(\"id\").reset_index(drop=True)\n",
    "        print(f\"Aligned on {len(common)} common ids for apples-to-apples comparison.\")\n",
    "\n",
    "    print(\"\\nScoring Reference set...\")\n",
    "    ref_preds = call_api_with_fallback(df_ref[\"comment_text\"].tolist())\n",
    "    print(\"Scoring Changed set...\")\n",
    "    cur_preds = call_api_with_fallback(df_cur[\"comment_text\"].tolist())\n",
    "\n",
    "    print(\"\\nMetrics on Reference set\")\n",
    "    f1w_ref, f1m_ref, ref_eval = evaluate(df_ref, ref_preds)\n",
    "    print(f\"F1 weighted: {f1w_ref:.4f} | F1 macro: {f1m_ref:.4f}\")\n",
    "\n",
    "    print(\"\\nMetrics on Changed set\")\n",
    "    f1w_cur, f1m_cur, cur_eval = evaluate(df_cur, cur_preds)\n",
    "    print(f\"F1 weighted: {f1w_cur:.4f} | F1 macro: {f1m_cur:.4f}\")\n",
    "\n",
    "    # Save per-row predictions\n",
    "    ref_csv = os.path.join(SAVE_DIR, \"preds_reference.csv\")\n",
    "    cur_csv = os.path.join(SAVE_DIR, \"preds_changed.csv\")\n",
    "    ref_eval.to_csv(ref_csv, index=False)\n",
    "    cur_eval.to_csv(cur_csv, index=False)\n",
    "    print(f\"Saved per-row predictions:\\n  {ref_csv}\\n  {cur_csv}\")\n",
    "\n",
    "    # Evidently report\n",
    "    build_and_save_reports(\n",
    "        ref_eval,\n",
    "        cur_eval,\n",
    "        report_name_slug=\"evidently_text_moderation_ref_vs_changed\",\n",
    "        push_cloud=True,\n",
    "    )\n",
    "\n",
    "    # Slide-friendly summary\n",
    "    delta_w = f1w_cur - f1w_ref\n",
    "    delta_m = f1m_cur - f1m_ref\n",
    "    print(\"\\n=== Summary ===\")\n",
    "    print(f\"Reference F1 weighted: {f1w_ref:.4f} | Changed: {f1w_cur:.4f} | Delta: {delta_w:+.4f}\")\n",
    "    print(f\"Reference F1 macro:    {f1m_ref:.4f} | Changed: {f1m_cur:.4f} | Delta: {delta_m:+.4f}\")\n",
    "    print(\"Open the HTML report to point at drift and classification sections.\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5349dad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test sets...\n",
      "Reference rows: 300\n",
      "Changed rows:   300\n",
      "Aligned on 300 common ids for apples-to-apples comparison.\n",
      "\n",
      "Scoring Reference set...\n",
      "  scored 160...\n",
      "  scored 300...\n",
      "Scoring Changed set...\n",
      "  scored 160...\n",
      "  scored 300...\n",
      "\n",
      "Metrics on Reference set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9786    0.9856    0.9821       278\n",
      "           1     0.8000    0.7273    0.7619        22\n",
      "\n",
      "    accuracy                         0.9667       300\n",
      "   macro avg     0.8893    0.8564    0.8720       300\n",
      "weighted avg     0.9655    0.9667    0.9659       300\n",
      "\n",
      "F1 weighted: 0.9659 | F1 macro: 0.8720\n",
      "\n",
      "Metrics on Changed set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9684    0.9928    0.9805       278\n",
      "           1     0.8667    0.5909    0.7027        22\n",
      "\n",
      "    accuracy                         0.9633       300\n",
      "   macro avg     0.9175    0.7919    0.8416       300\n",
      "weighted avg     0.9610    0.9633    0.9601       300\n",
      "\n",
      "F1 weighted: 0.9601 | F1 macro: 0.8416\n",
      "Saved per-row predictions:\n",
      "  monitoring_outputs/preds_reference.csv\n",
      "  monitoring_outputs/preds_changed.csv\n",
      "✓ Saved HTML report to: monitoring_outputs/evidently_text_moderation_ref_vs_changed.html\n",
      "✓ Uploaded run to Evidently Cloud.\n",
      "(Could not inline display: 'Report' object has no attribute 'as_html')\n",
      "\n",
      "=== Summary ===\n",
      "Reference F1 weighted: 0.9659 | Changed: 0.9601 | Delta: -0.0058\n",
      "Reference F1 macro:    0.8720 | Changed: 0.8416 | Delta: -0.0304\n",
      "Open the HTML report to point at drift and classification sections.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6924cfdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-comment-moderation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
